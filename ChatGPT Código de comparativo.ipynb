{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5de3ddb-df6f-4b9f-8776-2cbfcf39b8b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install xlrd>=2.0.1\n",
    "%pip install openpyxl\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a54dfe98-ec3b-4bf7-acb9-1001587cb268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- 1) Imports ----\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Se o spark não existir (em alguns ambientes locais), cria:\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ---- 2) Leitura do Excel ----\n",
    "# Suba o arquivo para /FileStore/tables/ no Databricks ou ajuste este caminho:\n",
    "file_path = \"/Workspace/Users/ahmad.jmazloum@gmail.com/Data__1_.xlsx\"\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# (Opcional) Converter para Spark para visualização no Databricks\n",
    "try:\n",
    "    df_spark = spark.createDataFrame(df)\n",
    "    display(df_spark)  # display() é nativo do Databricks\n",
    "except Exception as e:\n",
    "    print(\"display() indisponível fora do Databricks. Prosseguindo em pandas.\", e)\n",
    "\n",
    "# ---- 3) Inspeção inicial ----\n",
    "print(\"Formato (linhas, colunas):\", df.shape)\n",
    "print(\"\\nTipos de dados:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nPrimeiras linhas:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nValores ausentes por coluna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nResumo estatístico:\")\n",
    "print(df.describe())\n",
    "\n",
    "if \"Injury\" not in df.columns:\n",
    "    raise ValueError(\"A coluna 'Injury' não foi encontrada. Verifique o nome exato no Excel.\")\n",
    "\n",
    "print(\"\\nDistribuição de Injury (como está no arquivo):\")\n",
    "print(df[\"Injury\"].value_counts(dropna=False))\n",
    "\n",
    "# ---- 4) Limpeza e Normalização ----\n",
    "# Ajuste os nomes conforme seu arquivo. Estes são os nomes esperados:\n",
    "expected_cols = [\n",
    "    \"Rush Hour\",\n",
    "    \"Alcohol Involved\",\n",
    "    \"Work Zone\",\n",
    "    \"Align\",\n",
    "    \"Weekday\",\n",
    "    \"Accident at Intersection\",\n",
    "    \"Accident at Roadway\",\n",
    "    \"Speed Limit\",\n",
    "    \"Number of Vehicle Involved\",\n",
    "    \"Weather\",\n",
    "    \"Injury\"\n",
    "]\n",
    "\n",
    "# Conferência de colunas faltantes (apenas alerta)\n",
    "missing = [c for c in expected_cols if c not in df.columns]\n",
    "if missing:\n",
    "    print(\"\\n[AVISO] As colunas abaixo não foram encontradas no dataset e podem precisar de ajuste de nome:\")\n",
    "    print(missing)\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Mapear colunas de Yes/No -> 1/0 (se vierem como string)\n",
    "yes_no_cols = []\n",
    "for c in [\"Alcohol Involved\", \"Work Zone\", \"Weekday\", \"Weather\"]:\n",
    "    if c in df_clean.columns:\n",
    "        if df_clean[c].dtype == \"object\":\n",
    "            yes_no_cols.append(c)\n",
    "\n",
    "for c in yes_no_cols:\n",
    "    df_clean[c] = (\n",
    "        df_clean[c]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .map({\n",
    "            \"yes\": 1, \"y\": 1, \"sim\": 1, \"s\": 1, \"true\": 1, \"1\": 1,\n",
    "            \"no\": 0, \"n\": 0, \"nao\": 0, \"não\": 0, \"false\": 0, \"0\": 0\n",
    "        })\n",
    "    )\n",
    "\n",
    "# Converter Injury de {1,2} para {1,0} onde 1=ferido, 2=não ferido\n",
    "if df_clean[\"Injury\"].dtype != \"int64\" and df_clean[\"Injury\"].dtype != \"float64\":\n",
    "    df_clean[\"Injury\"] = pd.to_numeric(df_clean[\"Injury\"], errors=\"coerce\")\n",
    "\n",
    "df_clean[\"Injury\"] = df_clean[\"Injury\"].replace({2: 0})\n",
    "\n",
    "# Garantir numérico nas colunas principais\n",
    "cols_to_numeric = [c for c in expected_cols if c in df_clean.columns]\n",
    "for c in cols_to_numeric:\n",
    "    df_clean[c] = pd.to_numeric(df_clean[c], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nValores ausentes após conversões:\")\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "# Estratégia simples: remover linhas com NaN em qualquer coluna usada\n",
    "df_clean = df_clean.dropna(subset=[c for c in expected_cols if c in df_clean.columns]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nAmostra após limpeza:\")\n",
    "print(df_clean.head())\n",
    "\n",
    "# ---- 5) EDA (Análise Exploratória) ----\n",
    "# Distribuição do alvo\n",
    "try:\n",
    "    sns.countplot(x=\"Injury\", data=df_clean)\n",
    "    plt.title(\"Distribuição de Injury (0=sem ferido, 1=ferido)\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Plot não exibido (ambiente sem backend gráfico).\", e)\n",
    "\n",
    "# Correlação\n",
    "try:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    corr = df_clean.corr(numeric_only=True)\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"viridis\")\n",
    "    plt.title(\"Correlação entre variáveis\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Heatmap não exibido (ambiente sem backend gráfico).\", e)\n",
    "\n",
    "# Exemplo: taxa média de feridos por 'Alcohol Involved' (se existir)\n",
    "if \"Alcohol Involved\" in df_clean.columns:\n",
    "    print(\"\\nProbabilidade média de ferido por 'Alcohol Involved':\")\n",
    "    print(\n",
    "        df_clean.groupby(\"Alcohol Involved\")[\"Injury\"]\n",
    "        .mean()\n",
    "        .rename(\"Probabilidade de ferido\")\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        sns.barplot(\n",
    "            x=\"Alcohol Involved\",\n",
    "            y=\"Injury\",\n",
    "            data=df_clean,\n",
    "            estimator=np.mean\n",
    "        )\n",
    "        plt.title(\"Taxa média de feridos vs Álcool envolvido\")\n",
    "        plt.ylabel(\"Probabilidade de ferido\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Barplot não exibido.\", e)\n",
    "\n",
    "# ---- 6) Definir X e y ----\n",
    "feature_cols = [c for c in expected_cols if c != \"Injury\" and c in df_clean.columns]\n",
    "if not feature_cols:\n",
    "    raise ValueError(\"Nenhuma coluna preditora válida encontrada após a limpeza.\")\n",
    "\n",
    "X = df_clean[feature_cols]\n",
    "y = df_clean[\"Injury\"]\n",
    "\n",
    "# ---- 7) Train/Test split ----\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ---- 8) Treinamento: Regressão Logística e Random Forest ----\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_lr = logreg.predict(X_test)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# ---- 9) Avaliação ----\n",
    "def avaliar_modelo(y_true, y_pred, nome_modelo):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"\\n=== {nome_modelo} ===\")\n",
    "    print(f\"Acurácia : {acc:.3f}\")\n",
    "    print(f\"Precisão : {prec:.3f}\")\n",
    "    print(f\"Recall   : {rec:.3f}\")\n",
    "    print(f\"F1-score : {f1:.3f}\")\n",
    "    print(\"\\nRelatório de Classificação:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "  # Matriz de confusão\n",
    "    try:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cbar=False)\n",
    "        plt.title(f\"Matriz de Confusão - {nome_modelo}\")\n",
    "        plt.xlabel(\"Predito\")\n",
    "        plt.ylabel(\"Real\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Não foi possível exibir a matriz de confusão.\", e)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ChatGPT Código de comparativo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
